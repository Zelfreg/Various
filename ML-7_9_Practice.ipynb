{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, \\\n",
    "    RandomizedSearchCV, cross_val_score\n",
    "from hyperopt import hp, fmin, tpe, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Интро\n",
    "## 1.0 Кастомизация"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def on_da_screen(model)->float:\n",
    "    \"\"\"\n",
    "    Принимает на вход обученную модель. Рассчитывает и отображает F-меру тестовой выборки,\n",
    "    параметры переданной модели.\n",
    "\n",
    "    Params:    model: обученная модель\n",
    "\n",
    "    Returns:   F-мера тестовой выборки\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f'Лучшая комбинация:\\n {model.best_params_}\\n')\n",
    "    except: pass\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    f_score = f1_score(y_test, y_test_pred)\n",
    "    print(f'F1-мера на тестовой части: {round(f_score, 3)}')\n",
    "    return f_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Подгрузка данных"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3751 entries, 0 to 3750\n",
      "Columns: 1777 entries, Activity to D1776\n",
      "dtypes: float64(942), int64(835)\n",
      "memory usage: 50.9 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "None"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "   Activity        D1        D2    D3   D4        D5        D6        D7  \\\n0         1  0.000000  0.497009  0.10  0.0  0.132956  0.678031  0.273166   \n1         1  0.366667  0.606291  0.05  0.0  0.111209  0.803455  0.106105   \n2         1  0.033300  0.480124  0.00  0.0  0.209791  0.610350  0.356453   \n3         1  0.000000  0.538825  0.00  0.5  0.196344  0.724230  0.235606   \n4         0  0.100000  0.517794  0.00  0.0  0.494734  0.781422  0.154361   \n\n         D8        D9  ...  D1767  D1768  D1769  D1770  D1771  D1772  D1773  \\\n0  0.585445  0.743663  ...      0      0      0      0      0      0      0   \n1  0.411754  0.836582  ...      1      1      1      1      0      1      0   \n2  0.517720  0.679051  ...      0      0      0      0      0      0      0   \n3  0.288764  0.805110  ...      0      0      0      0      0      0      0   \n4  0.303809  0.812646  ...      0      0      0      0      0      0      0   \n\n   D1774  D1775  D1776  \n0      0      0      0  \n1      0      1      0  \n2      0      0      0  \n3      0      0      0  \n4      0      0      0  \n\n[5 rows x 1777 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Activity</th>\n      <th>D1</th>\n      <th>D2</th>\n      <th>D3</th>\n      <th>D4</th>\n      <th>D5</th>\n      <th>D6</th>\n      <th>D7</th>\n      <th>D8</th>\n      <th>D9</th>\n      <th>...</th>\n      <th>D1767</th>\n      <th>D1768</th>\n      <th>D1769</th>\n      <th>D1770</th>\n      <th>D1771</th>\n      <th>D1772</th>\n      <th>D1773</th>\n      <th>D1774</th>\n      <th>D1775</th>\n      <th>D1776</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.497009</td>\n      <td>0.10</td>\n      <td>0.0</td>\n      <td>0.132956</td>\n      <td>0.678031</td>\n      <td>0.273166</td>\n      <td>0.585445</td>\n      <td>0.743663</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.366667</td>\n      <td>0.606291</td>\n      <td>0.05</td>\n      <td>0.0</td>\n      <td>0.111209</td>\n      <td>0.803455</td>\n      <td>0.106105</td>\n      <td>0.411754</td>\n      <td>0.836582</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0.033300</td>\n      <td>0.480124</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.209791</td>\n      <td>0.610350</td>\n      <td>0.356453</td>\n      <td>0.517720</td>\n      <td>0.679051</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.538825</td>\n      <td>0.00</td>\n      <td>0.5</td>\n      <td>0.196344</td>\n      <td>0.724230</td>\n      <td>0.235606</td>\n      <td>0.288764</td>\n      <td>0.805110</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.100000</td>\n      <td>0.517794</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.494734</td>\n      <td>0.781422</td>\n      <td>0.154361</td>\n      <td>0.303809</td>\n      <td>0.812646</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1777 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/_train_sem09.csv')\n",
    "data = df.copy()\n",
    "display(data.info())\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Предобработка\n",
    "Сделана."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Построение моделей\n",
    "Перед построением моделей разобъём датасет на тестовую и тренировочную части."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1    0.542255\n0    0.457745\nName: Activity, dtype: float64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.drop(columns='Activity')\n",
    "y = data['Activity']\n",
    "# оценим сбалансированность целевой переменной\n",
    "y.hist()\n",
    "y.value_counts(normalize=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Разница приемлемая -- обойдёмся без стратификации."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Логистическая регрессия\n",
    "### 2.1.1 Базовая модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера на тестовой части: 0.789\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=5000)\n",
    "lr.fit(X_train, y_train)\n",
    "pred_y_test_base = lr.predict(X_test)\n",
    "f1_score_base_lg = on_da_screen(lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2 Grid search\n",
    "Так как данные уже нормализованны, будем использовать алгоритмы реализующие такую предобработку данных: sag, saga и дополнительно lbfgs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 3s\n",
      "Wall time: 10min 56s\n",
      "Лучшая комбинация:\n",
      " {'C': 0.5, 'penalty': 'l1', 'solver': 'saga'}\n",
      "\n",
      "F1-мера на тестовой части: 0.797\n"
     ]
    }
   ],
   "source": [
    "# Параметры поиска\n",
    "param_grid_lr = [{'penalty': ['l1', 'l2', 'none'],\n",
    "                  'solver': ['saga'],\n",
    "                  'C': [0.01, 0.5, 1]},\n",
    "                 {'penalty': ['elasticnet'],\n",
    "                  'solver': ['saga'],\n",
    "                  'C': [0.01, 0.5, 1],\n",
    "                  'l1_ratio': [0.25, 0.5, 0.75]},\n",
    "                 {'penalty': ['l2', 'none'],\n",
    "                  'solver': ['lbfgs', 'sag'],\n",
    "                  'C': [0.01, 0.5, 1]}]\n",
    "# Cоздаём объект класса\n",
    "grid_search_lr = GridSearchCV(estimator=LogisticRegression(max_iter = 2000,\n",
    "                                                           random_state=42),\n",
    "                              param_grid=param_grid_lr,\n",
    "                              scoring='f1',\n",
    "                              n_jobs=-1)\n",
    "%time grid_search_lr.fit(X_train, y_train)\n",
    "f1_score_search_grid_lr = on_da_screen(grid_search_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.3 Random search\n",
    "Так как этот метод выполняется быстрее чем grid search, расширим набор параметров С и l1_ratio."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14.3 s\n",
      "Wall time: 4min\n",
      "Лучшая комбинация:\n",
      " {'solver': 'saga', 'penalty': 'elasticnet', 'l1_ratio': 0.4, 'C': 0.109}\n",
      "\n",
      "F1-мера на тестовой части: 0.8\n"
     ]
    }
   ],
   "source": [
    "C = list(np.linspace(0.01, 1, 11))\n",
    "param_rand = [{'penalty': ['l1', 'l2', 'none'],\n",
    "               'solver': ['saga'],\n",
    "               'C': C},\n",
    "              {'penalty': ['elasticnet'],\n",
    "               'solver': ['saga'],\n",
    "               'C': C,\n",
    "               'l1_ratio': list(np.linspace(0.1, 0.9, 9))},\n",
    "              {'penalty': ['l2', 'none'],\n",
    "               'solver': ['lbfgs', 'sag'],\n",
    "               'C': C}]\n",
    "rand_search_lr = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=2000, random_state=42),\n",
    "    param_distributions=param_rand,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "%time rand_search_lr.fit(X_train, y_train)\n",
    "f1_score_rand_search_lr = on_da_screen(rand_search_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.4 Hyperopt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:50<00:00, 26.54s/trial, best loss: -0.7827501176724684]\n",
      "Наилучшие значения гиперпараметров {'C': 0.11381056696717652, 'l1_ratio': 0.8349780173355015, 'penalty': 1, 'solver': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера на тестовой части: 0.795\n"
     ]
    }
   ],
   "source": [
    "def hyperopt_lr(params)->float:\n",
    "    \"\"\"\n",
    "    Функция для минимизации в библиотеке hyperopt. Принимает гиперпараметры\n",
    "    модели. Рассчитывает F-меру, используя кроссвалидацию по умолчанию(kf=5)\n",
    "    , и возвращает её со знаком минус, так мы ищем наименьшее значение.\n",
    "\n",
    "     Params:\n",
    "            params: Набор гиперпараметров модели, передаваемый из функции\n",
    "                    fmin()\n",
    "\n",
    "     Return:\n",
    "            F-мера со знаком минус\n",
    "    \"\"\"\n",
    "\n",
    "    # проверка параметров на совместимость:\n",
    "    sag_and_lbfgs_penalty = ['l2', 'none']\n",
    "    if params['solver'] == 'sag' or params['solver'] == 'lbfgs':\n",
    "        if params['penalty'] in sag_and_lbfgs_penalty:\n",
    "            del params['l1_ratio']\n",
    "        else: return 0\n",
    "    else:\n",
    "        if params['penalty'] != 'elasticnet':\n",
    "            del params['l1_ratio']\n",
    "\n",
    "    if params['penalty'] == 'none':\n",
    "        del params['C']\n",
    "\n",
    "    model = LogisticRegression(**params, max_iter=5000, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=\"f1\",\n",
    "                            n_jobs=-1).mean()\n",
    "    # метрику необходимо минимизировать\n",
    "    return -score\n",
    "\n",
    "\n",
    "trials_lr = Trials() # логирования результатов\n",
    "penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "solver = ['sag', 'saga', 'lbfgs']\n",
    "space_lr = {\n",
    "    'penalty': hp.choice('penalty', penalty),\n",
    "    'solver': hp.choice('solver', solver),\n",
    "    'C': hp.uniform('C', 0.01, 1),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0.1, 0.9)\n",
    "}\n",
    "best_lr_hyperopt = fmin(hyperopt_lr,\n",
    "                        space=space_lr,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=20,\n",
    "                        trials=trials_lr,\n",
    "                        rstate=np.random.RandomState(42)\n",
    "                        )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best_lr_hyperopt))\n",
    "# заменяем индексы на их значения в соответствующих списках\n",
    "best_lr_hyperopt['penalty'] = penalty[best_lr_hyperopt['penalty']]\n",
    "best_lr_hyperopt['solver'] = solver[best_lr_hyperopt['solver']]\n",
    "# модель с полученными гиперпараметрами\n",
    "hyperopt_lr_best = LogisticRegression(**best_lr_hyperopt, max_iter=5000,\n",
    "                                      random_state=42)\n",
    "hyperopt_lr_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_hyperopt_lr = on_da_screen(hyperopt_lr_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "+20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [10:03<00:00, 30.17s/trial, best loss: -0.7864384716009365]\n",
      "Наилучшие значения гиперпараметров {'C': 0.045017900498050614, 'l1_ratio': 0.3192001651038195, 'penalty': 1, 'solver': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера на тестовой части: 0.795\n"
     ]
    }
   ],
   "source": [
    "best_lr_hyperopt_extra = fmin(hyperopt_lr,\n",
    "                        space=space_lr,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=40,\n",
    "                        trials=trials_lr,\n",
    "                        rstate=np.random.RandomState(42)\n",
    "                        )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best_lr_hyperopt_extra))\n",
    "# заменяем индексы на их значения в соответствующих списках\n",
    "best_lr_hyperopt_extra['penalty'] = penalty[best_lr_hyperopt_extra['penalty']]\n",
    "best_lr_hyperopt_extra['solver'] = solver[best_lr_hyperopt_extra['solver']]\n",
    "# модель с полученными гиперпараметрами\n",
    "hyperopt_lr_best_extra = LogisticRegression(**best_lr_hyperopt_extra,\n",
    "                                            max_iter=5000,\n",
    "                                            random_state=42)\n",
    "hyperopt_lr_best_extra.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_hyperopt_lr_extra = on_da_screen(hyperopt_lr_best_extra)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Немного поднялась метрика тренировочно части, но на тестовой - это никак не отразилось."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.5 Optuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-19 22:04:46,537]\u001B[0m A new study created in memory with name: Test run\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:04:46,538]\u001B[0m Trial 0 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.66, 'penalty': 'elasticnet', 'l1_ratio': 0.6000000000000001}. Best is trial 0 with value: 0.0.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:04:46,539]\u001B[0m Trial 1 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.060000000000000005, 'penalty': 'elasticnet', 'l1_ratio': 0.7500000000000001}. Best is trial 0 with value: 0.0.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:07:50,084]\u001B[0m Trial 2 finished with value: 0.7805583718514042 and parameters: {'solver': 'saga', 'C': 0.91, 'penalty': 'l1', 'l1_ratio': 0.6500000000000001}. Best is trial 2 with value: 0.7805583718514042.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:12:55,236]\u001B[0m Trial 3 finished with value: 0.7505277127666353 and parameters: {'solver': 'sag', 'C': 0.41000000000000003, 'penalty': 'none', 'l1_ratio': 0.5}. Best is trial 2 with value: 0.7805583718514042.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:13:06,984]\u001B[0m Trial 4 finished with value: 0.7712489083891418 and parameters: {'solver': 'saga', 'C': 0.01, 'penalty': 'elasticnet', 'l1_ratio': 0.05}. Best is trial 2 with value: 0.7805583718514042.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:13:06,986]\u001B[0m Trial 5 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.31000000000000005, 'penalty': 'l1', 'l1_ratio': 0.95}. Best is trial 2 with value: 0.7805583718514042.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:15:59,137]\u001B[0m Trial 6 finished with value: 0.782374116874305 and parameters: {'solver': 'saga', 'C': 0.76, 'penalty': 'l1', 'l1_ratio': 0.1}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:15:59,138]\u001B[0m Trial 7 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.41000000000000003, 'penalty': 'elasticnet', 'l1_ratio': 0.5}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:16:30,063]\u001B[0m Trial 8 finished with value: 0.7699966651328591 and parameters: {'solver': 'sag', 'C': 0.76, 'penalty': 'l2', 'l1_ratio': 0.15000000000000002}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:16:47,113]\u001B[0m Trial 9 finished with value: 0.7273489086785705 and parameters: {'solver': 'lbfgs', 'C': 0.91, 'penalty': 'none', 'l1_ratio': 0.8500000000000001}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:19:30,679]\u001B[0m Trial 10 finished with value: 0.7814067275566542 and parameters: {'solver': 'saga', 'C': 0.66, 'penalty': 'l1', 'l1_ratio': 0.35000000000000003}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:22:05,730]\u001B[0m Trial 11 finished with value: 0.7821462535175424 and parameters: {'solver': 'saga', 'C': 0.6100000000000001, 'penalty': 'l1', 'l1_ratio': 0.3}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:24:44,286]\u001B[0m Trial 12 finished with value: 0.7821462535175424 and parameters: {'solver': 'saga', 'C': 0.6100000000000001, 'penalty': 'l1', 'l1_ratio': 0.3}. Best is trial 6 with value: 0.782374116874305.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:27:49,124]\u001B[0m Trial 13 finished with value: 0.782376753215205 and parameters: {'solver': 'saga', 'C': 0.81, 'penalty': 'l1', 'l1_ratio': 0.25}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:27:54,813]\u001B[0m Trial 14 finished with value: 0.7704888323981046 and parameters: {'solver': 'lbfgs', 'C': 0.81, 'penalty': 'l2', 'l1_ratio': 0.15000000000000002}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:31:17,009]\u001B[0m Trial 15 finished with value: 0.7795063371400853 and parameters: {'solver': 'saga', 'C': 0.96, 'penalty': 'l1', 'l1_ratio': 0.05}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:34:19,767]\u001B[0m Trial 16 finished with value: 0.782376753215205 and parameters: {'solver': 'saga', 'C': 0.81, 'penalty': 'l1', 'l1_ratio': 0.2}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:35:30,348]\u001B[0m Trial 17 finished with value: 0.7815043078932705 and parameters: {'solver': 'saga', 'C': 0.21000000000000002, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:35:35,557]\u001B[0m Trial 18 finished with value: 0.7727369523832007 and parameters: {'solver': 'lbfgs', 'C': 0.51, 'penalty': 'l2', 'l1_ratio': 0.2}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:40:53,997]\u001B[0m Trial 19 finished with value: 0.75152736829291 and parameters: {'solver': 'saga', 'C': 0.81, 'penalty': 'none', 'l1_ratio': 0.25}. Best is trial 13 with value: 0.782376753215205.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'solver': 'saga', 'C': 0.81, 'penalty': 'l1', 'l1_ratio': 0.25}\n",
      "f1_score на обучающем наборе: 0.78\n",
      "F1-мера на тестовой части: 0.789\n"
     ]
    }
   ],
   "source": [
    "def optuna_lr(trial)->float:\n",
    "    \"\"\"\n",
    "    Задаёт пространство гиперпараметров и обучает модель.\n",
    "    Возвращает F-меру модели на обучающей выборке. Нежизнеспособным\n",
    "    комбинациям присваивается нулевое значение.\n",
    "    \"\"\"\n",
    "    solver_opt = trial.suggest_categorical('solver', ['sag', 'saga', 'lbfgs'])\n",
    "    c_opt = trial.suggest_float('C', 0.01, 0.96, step=0.05)\n",
    "    penalty_opt = trial.suggest_categorical(\n",
    "            'penalty', ['l1', 'l2', 'elasticnet', 'none']\n",
    "    )\n",
    "    l1_ratio_opt = trial.suggest_float('l1_ratio', 0.05, 0.95, step=0.05)\n",
    "    try:\n",
    "        model_opt = LogisticRegression(max_iter=5000,\n",
    "                                       random_state=42,\n",
    "                                       solver=solver_opt,\n",
    "                                       C=c_opt,\n",
    "                                       penalty=penalty_opt,\n",
    "                                       l1_ratio=l1_ratio_opt)\n",
    "        model_opt.fit(X_train, y_train)\n",
    "        score = cross_val_score(model_opt, X_train, y_train, scoring=\"f1\",\n",
    "                                n_jobs=-1).mean()\n",
    "    except ValueError:\n",
    "        score = 0\n",
    "    return score\n",
    "\n",
    "\n",
    "# Ищем лучшую комбинацию гиперпараметров при максимизации метрики:\n",
    "study = optuna.create_study(study_name='Test run', direction=\"maximize\")\n",
    "study.optimize(optuna_lr, n_trials=20)\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))\n",
    "# модель с полученными гиперпараметрами\n",
    "optuna_lr_best = LogisticRegression(**study.best_params, max_iter=5000,\n",
    "                                    random_state=42)\n",
    "optuna_lr_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_optuna_lr = on_da_screen(optuna_lr_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модель с полученными параметрами показывает результат схожий с базовым. Попробуем улучшить его, для этого поднимем к-во итераций до 40."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:45:14,594]\u001B[0m Trial 20 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:48:19,136]\u001B[0m Trial 21 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:51:33,599]\u001B[0m Trial 22 finished with value: 0.7805583718514042 and parameters: {'solver': 'saga', 'C': 0.91, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:55:06,182]\u001B[0m Trial 23 finished with value: 0.7795063371400853 and parameters: {'solver': 'saga', 'C': 0.96, 'penalty': 'l1', 'l1_ratio': 0.45}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 22:58:07,506]\u001B[0m Trial 24 finished with value: 0.7813695635338181 and parameters: {'solver': 'saga', 'C': 0.7100000000000001, 'penalty': 'l1', 'l1_ratio': 0.6000000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 22:58:07,510]\u001B[0m Trial 25 finished with value: 0.0 and parameters: {'solver': 'lbfgs', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:00:50,234]\u001B[0m Trial 26 finished with value: 0.7818752679066195 and parameters: {'solver': 'saga', 'C': 0.56, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:06:13,354]\u001B[0m Trial 27 finished with value: 0.75152736829291 and parameters: {'solver': 'saga', 'C': 0.7100000000000001, 'penalty': 'none', 'l1_ratio': 0.3}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:06:49,567]\u001B[0m Trial 28 finished with value: 0.7698675044165271 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l2', 'l1_ratio': 0.2}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:06:49,571]\u001B[0m Trial 29 finished with value: 0.0 and parameters: {'solver': 'lbfgs', 'C': 0.7100000000000001, 'penalty': 'elasticnet', 'l1_ratio': 0.7000000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:09:51,724]\u001B[0m Trial 30 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:13:00,299]\u001B[0m Trial 31 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:16:25,477]\u001B[0m Trial 32 finished with value: 0.7795063371400853 and parameters: {'solver': 'saga', 'C': 0.96, 'penalty': 'l1', 'l1_ratio': 0.6000000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:19:33,989]\u001B[0m Trial 33 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:20:10,824]\u001B[0m Trial 34 finished with value: 0.7808350369521391 and parameters: {'solver': 'saga', 'C': 0.16000000000000003, 'penalty': 'l1', 'l1_ratio': 0.7500000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:20:10,828]\u001B[0m Trial 35 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.91, 'penalty': 'elasticnet', 'l1_ratio': 0.45}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:23:08,051]\u001B[0m Trial 36 finished with value: 0.782374116874305 and parameters: {'solver': 'saga', 'C': 0.76, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:25:53,589]\u001B[0m Trial 37 finished with value: 0.7814067275566542 and parameters: {'solver': 'saga', 'C': 0.66, 'penalty': 'l1', 'l1_ratio': 0.6500000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:31:05,147]\u001B[0m Trial 38 finished with value: 0.7505277127666353 and parameters: {'solver': 'sag', 'C': 0.41000000000000003, 'penalty': 'none', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:32:30,660]\u001B[0m Trial 39 finished with value: 0.7772339702286882 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'elasticnet', 'l1_ratio': 0.45}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.4}\n",
      "f1_score на обучающем наборе: 0.78\n",
      "F1-мера на тестовой части: 0.785\n"
     ]
    }
   ],
   "source": [
    "study.optimize(optuna_lr, n_trials=20)\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))\n",
    "# модель с полученными гиперпараметрами\n",
    "optuna_lr_best_extra = LogisticRegression(**study.best_params, max_iter=5000,\n",
    "                                          random_state=42)\n",
    "optuna_lr_best_extra.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_optuna_lr_extra = on_da_screen(optuna_lr_best_extra)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Хм метрика на тестовой ещё ниже чем на двадцати итерациях... Попробуем ещё +20 итераций."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:37:16,762]\u001B[0m Trial 40 finished with value: 0.7795063371400853 and parameters: {'solver': 'saga', 'C': 0.96, 'penalty': 'l1', 'l1_ratio': 0.7000000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:40:28,888]\u001B[0m Trial 41 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.5}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:43:31,463]\u001B[0m Trial 42 finished with value: 0.782374116874305 and parameters: {'solver': 'saga', 'C': 0.76, 'penalty': 'l1', 'l1_ratio': 0.6500000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:46:45,092]\u001B[0m Trial 43 finished with value: 0.7805583718514042 and parameters: {'solver': 'saga', 'C': 0.91, 'penalty': 'l1', 'l1_ratio': 0.5}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:46:45,096]\u001B[0m Trial 44 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.35000000000000003}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:49:50,070]\u001B[0m Trial 45 finished with value: 0.782376753215205 and parameters: {'solver': 'saga', 'C': 0.81, 'penalty': 'l1', 'l1_ratio': 0.5}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:50:21,866]\u001B[0m Trial 46 finished with value: 0.772344057583913 and parameters: {'solver': 'saga', 'C': 0.66, 'penalty': 'l2', 'l1_ratio': 0.35000000000000003}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:53:19,852]\u001B[0m Trial 47 finished with value: 0.782374116874305 and parameters: {'solver': 'saga', 'C': 0.76, 'penalty': 'l1', 'l1_ratio': 0.55}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:53:19,856]\u001B[0m Trial 48 finished with value: 0.0 and parameters: {'solver': 'lbfgs', 'C': 0.91, 'penalty': 'l1', 'l1_ratio': 0.95}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-19 23:58:52,275]\u001B[0m Trial 49 finished with value: 0.75152736829291 and parameters: {'solver': 'saga', 'C': 0.6100000000000001, 'penalty': 'none', 'l1_ratio': 0.45}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "\u001B[32m[I 2022-08-19 23:58:52,280]\u001B[0m Trial 50 finished with value: 0.0 and parameters: {'solver': 'sag', 'C': 0.76, 'penalty': 'elasticnet', 'l1_ratio': 0.8500000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:02:00,334]\u001B[0m Trial 51 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.5}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:05:04,583]\u001B[0m Trial 52 finished with value: 0.782376753215205 and parameters: {'solver': 'saga', 'C': 0.81, 'penalty': 'l1', 'l1_ratio': 0.5}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:08:13,376]\u001B[0m Trial 53 finished with value: 0.7824490517772814 and parameters: {'solver': 'saga', 'C': 0.8600000000000001, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:11:38,709]\u001B[0m Trial 54 finished with value: 0.7795063371400853 and parameters: {'solver': 'saga', 'C': 0.96, 'penalty': 'l1', 'l1_ratio': 0.6000000000000001}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:14:48,276]\u001B[0m Trial 55 finished with value: 0.7805583718514042 and parameters: {'solver': 'saga', 'C': 0.91, 'penalty': 'l1', 'l1_ratio': 0.4}. Best is trial 20 with value: 0.7824490517772814.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:16:48,654]\u001B[0m Trial 56 finished with value: 0.783530988573515 and parameters: {'solver': 'saga', 'C': 0.31000000000000005, 'penalty': 'l1', 'l1_ratio': 0.35000000000000003}. Best is trial 56 with value: 0.783530988573515.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:18:25,732]\u001B[0m Trial 57 finished with value: 0.7820032699608375 and parameters: {'solver': 'saga', 'C': 0.26, 'penalty': 'l1', 'l1_ratio': 0.35000000000000003}. Best is trial 56 with value: 0.783530988573515.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "\u001B[32m[I 2022-08-20 00:18:47,699]\u001B[0m Trial 58 finished with value: 0.7799921548046879 and parameters: {'solver': 'saga', 'C': 0.31000000000000005, 'penalty': 'l2', 'l1_ratio': 0.3}. Best is trial 56 with value: 0.783530988573515.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:18:47,703]\u001B[0m Trial 59 finished with value: 0.0 and parameters: {'solver': 'lbfgs', 'C': 0.11, 'penalty': 'l1', 'l1_ratio': 0.45}. Best is trial 56 with value: 0.783530988573515.\u001B[0m\n",
      "C:\\Users\\gosta\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'solver': 'saga', 'C': 0.31000000000000005, 'penalty': 'l1', 'l1_ratio': 0.35000000000000003}\n",
      "f1_score на обучающем наборе: 0.78\n",
      "F1-мера на тестовой части: 0.801\n"
     ]
    }
   ],
   "source": [
    "study.optimize(optuna_lr, n_trials=20)\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))\n",
    "# модель с полученными гиперпараметрами\n",
    "optuna_lr_best_extra_2 = LogisticRegression(**study.best_params, max_iter=5000,\n",
    "                                          random_state=42)\n",
    "optuna_lr_best_extra_2.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_optuna_lr_extra_2 = on_da_screen(optuna_lr_best_extra_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Случайный лес\n",
    "### 2.2.1 Базовая модель"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера на тестовой части: 0.828\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "f1_score_base_rf = on_da_screen(rf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.2 Grid search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшая комбинация:\n",
      " {'criterion': 'gini', 'max_depth': 6, 'min_samples_leaf': 5, 'n_estimators': 600}\n",
      "\n",
      "F1-мера на тестовой части: 0.798\n"
     ]
    }
   ],
   "source": [
    "# пространство гиперпараметров\n",
    "param_grid_rf = {'n_estimators': [100, 200, 400, 600, 800],\n",
    "                 'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                 'max_depth': [2, 4, 6],\n",
    "                 'min_samples_leaf': [5, 10, 15]}\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(n_jobs=-1,\n",
    "                                                               random_state=42),\n",
    "                              param_grid=param_grid_rf,\n",
    "                              scoring='f1',\n",
    "                              n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "f1_score_search_grid_rf = on_da_screen(grid_search_rf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.3 Random search"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19.3 s\n",
      "Wall time: 39.3 s\n",
      "Лучшая комбинация:\n",
      " {'n_estimators': 1600, 'min_samples_leaf': 5, 'max_depth': 18, 'criterion': 'log_loss'}\n",
      "\n",
      "F1-мера на тестовой части: 0.839\n"
     ]
    }
   ],
   "source": [
    "param_rand_rf = {'n_estimators': [100, 200, 400, 800, 1600],\n",
    "                 'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "                 'max_depth': list(range(6, 42, 2)),\n",
    "                 'min_samples_leaf': list(range(1, 16))}\n",
    "rand_search_rf = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    param_distributions=param_rand_rf,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "%time rand_search_rf.fit(X_train, y_train)\n",
    "f1_score_rand_search_rf = on_da_screen(rand_search_rf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.4 Hyperopt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def hyperopt_rf(params)->float:\n",
    "    \"\"\"\n",
    "    Функция для минимизации в библиотеке hyperopt. Принимает гиперпараметры\n",
    "    модели. Рассчитывает F-меру, используя кроссвалидацию по умолчанию(kf=5)\n",
    "    , и возвращает её со знаком минус, так мы ищем наименьшее значение.\n",
    "\n",
    "     Params:\n",
    "            params: Набор гиперпараметров модели, передаваемый из функции\n",
    "                    fmin()\n",
    "\n",
    "     Return:\n",
    "            F-мера со знаком минус\n",
    "    \"\"\"\n",
    "    params = {'n_estimators': int(params['n_estimators']),\n",
    "              'max_depth': int(params['max_depth']),\n",
    "              'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "              'criterion': params['criterion']}\n",
    "    model = RandomForestClassifier(**params, n_jobs=-1, random_state=42)\n",
    "    score = cross_val_score(model, X_train, y_train, scoring=\"f1\",\n",
    "                            n_jobs=-1).mean()\n",
    "    # метрику необходимо минимизировать\n",
    "    return -score\n",
    "\n",
    "\n",
    "trials_rf = Trials() # логирования результатов\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "space_rf = {'n_estimators': hp.quniform('n_estimators', 100, 2000, 50),\n",
    "            'max_depth' : hp.quniform('max_depth', 6, 40, 1),\n",
    "            'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 15, 1),\n",
    "            'criterion': hp.choice('criterion', criterion)}\n",
    "best_rf_hyperopt = fmin(hyperopt_rf,\n",
    "                        space=space_rf,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=20,\n",
    "                        trials=trials_rf,\n",
    "                        rstate=np.random.RandomState(42)\n",
    "                        )\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(best_rf_hyperopt))\n",
    "best_rf_hyperopt = {x: int(best_rf_hyperopt[x]) for x in best_rf_hyperopt\n",
    "    .keys()}\n",
    "# заменяем индекс на его значение\n",
    "best_rf_hyperopt['criterion'] = criterion[best_rf_hyperopt['criterion']]\n",
    "# модель с полученными гиперпараметрами\n",
    "hyperopt_rf_best = RandomForestClassifier(**best_rf_hyperopt, n_jobs=-1,\n",
    "                                          random_state=42)\n",
    "hyperopt_rf_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_hyperopt_rf = on_da_screen(hyperopt_rf_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:11<00:00,  6.59s/trial, best loss: -0.8091294972610672]\n",
      "Наилучшие значения гиперпараметров {'criterion': 2, 'max_depth': 35.0, 'min_samples_leaf': 1.0, 'n_estimators': 1100.0}\n",
      "F1-мера на тестовой части: 0.832\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Попробуем улучшить, для этого поднимем к-во итераций до 40."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:59<00:00,  2.96s/trial, best loss: -0.8139521366663125]\n",
      "Наилучшие значения гиперпараметров {'criterion': 'gini', 'max_depth': 28, 'min_samples_leaf': 1, 'n_estimators': 250}\n",
      "F1-мера на тестовой части: 0.834\n"
     ]
    }
   ],
   "source": [
    "hyperopt_extra_rf = fmin(hyperopt_rf,\n",
    "                         space=space_rf,\n",
    "                         algo=tpe.suggest,\n",
    "                         max_evals=40,\n",
    "                         trials=trials_rf,\n",
    "                         rstate=np.random.RandomState(42)\n",
    "                        )\n",
    "extra = {x: int(hyperopt_extra_rf[x]) for x in hyperopt_extra_rf.keys()}\n",
    "# заменяем индекс на его значение\n",
    "extra['criterion'] = criterion[extra['criterion']]\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(extra))\n",
    "# модель с полученными гиперпараметрами\n",
    "hyperopt_rf_best = RandomForestClassifier(**extra, n_jobs=-1,\n",
    "                                          random_state=42)\n",
    "hyperopt_rf_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_hyperopt_rf_extra = on_da_screen(hyperopt_rf_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "до 60"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<?, ?trial/s, best loss=?]\n",
      "Наилучшие значения гиперпараметров {'criterion': 'gini', 'max_depth': 28, 'min_samples_leaf': 1, 'n_estimators': 250}\n",
      "F1-мера на тестовой части: 0.834\n"
     ]
    }
   ],
   "source": [
    "hyperopt_extra_rf = fmin(hyperopt_rf,\n",
    "                         space=space_rf,\n",
    "                         algo=tpe.suggest,\n",
    "                         max_evals=40,\n",
    "                         trials=trials_rf,\n",
    "                         rstate=np.random.RandomState(42)\n",
    "                        )\n",
    "extra = {x: int(hyperopt_extra_rf[x]) for x in hyperopt_extra_rf.keys()}\n",
    "# заменяем индекс на его значение\n",
    "extra['criterion'] = criterion[extra['criterion']]\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(extra))\n",
    "# модель с полученными гиперпараметрами\n",
    "hyperopt_rf_best = RandomForestClassifier(**extra, n_jobs=-1,\n",
    "                                          random_state=42)\n",
    "hyperopt_rf_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_hyperopt_rf_extra_60 = on_da_screen(hyperopt_rf_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2.5 Optuna"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-08-20 00:26:01,979]\u001B[0m A new study created in memory with name: Test run\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:09,405]\u001B[0m Trial 0 finished with value: 0.8043577705264854 and parameters: {'criterion': 'log_loss', 'n_estimators': 950, 'max_depth': 28, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8043577705264854.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:11,556]\u001B[0m Trial 1 finished with value: 0.7864884088411424 and parameters: {'criterion': 'gini', 'n_estimators': 300, 'max_depth': 11, 'min_samples_leaf': 11}. Best is trial 0 with value: 0.8043577705264854.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:16,641]\u001B[0m Trial 2 finished with value: 0.7876045535922385 and parameters: {'criterion': 'log_loss', 'n_estimators': 850, 'max_depth': 31, 'min_samples_leaf': 15}. Best is trial 0 with value: 0.8043577705264854.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:24,040]\u001B[0m Trial 3 finished with value: 0.8046411104244784 and parameters: {'criterion': 'entropy', 'n_estimators': 950, 'max_depth': 30, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:30,594]\u001B[0m Trial 4 finished with value: 0.7719792712305101 and parameters: {'criterion': 'gini', 'n_estimators': 1450, 'max_depth': 7, 'min_samples_leaf': 13}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:35,539]\u001B[0m Trial 5 finished with value: 0.7877337712767897 and parameters: {'criterion': 'entropy', 'n_estimators': 800, 'max_depth': 19, 'min_samples_leaf': 15}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:41,446]\u001B[0m Trial 6 finished with value: 0.7743281031995282 and parameters: {'criterion': 'entropy', 'n_estimators': 1150, 'max_depth': 7, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:44,221]\u001B[0m Trial 7 finished with value: 0.8033109346128089 and parameters: {'criterion': 'gini', 'n_estimators': 350, 'max_depth': 19, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:47,585]\u001B[0m Trial 8 finished with value: 0.7895266737780563 and parameters: {'criterion': 'entropy', 'n_estimators': 500, 'max_depth': 33, 'min_samples_leaf': 11}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:26:57,760]\u001B[0m Trial 9 finished with value: 0.8040962024763616 and parameters: {'criterion': 'log_loss', 'n_estimators': 1400, 'max_depth': 33, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.8046411104244784.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:27:15,238]\u001B[0m Trial 10 finished with value: 0.8115053516440011 and parameters: {'criterion': 'entropy', 'n_estimators': 1950, 'max_depth': 40, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8115053516440011.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:27:32,292]\u001B[0m Trial 11 finished with value: 0.811636473740694 and parameters: {'criterion': 'entropy', 'n_estimators': 1900, 'max_depth': 40, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.811636473740694.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:27:50,042]\u001B[0m Trial 12 finished with value: 0.8115053516440011 and parameters: {'criterion': 'entropy', 'n_estimators': 1950, 'max_depth': 40, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.811636473740694.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:28:07,421]\u001B[0m Trial 13 finished with value: 0.8115053516440011 and parameters: {'criterion': 'entropy', 'n_estimators': 1950, 'max_depth': 40, 'min_samples_leaf': 1}. Best is trial 11 with value: 0.811636473740694.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:28:22,007]\u001B[0m Trial 14 finished with value: 0.8166661331917211 and parameters: {'criterion': 'entropy', 'n_estimators': 1650, 'max_depth': 37, 'min_samples_leaf': 2}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:28:35,652]\u001B[0m Trial 15 finished with value: 0.8121123526411665 and parameters: {'criterion': 'entropy', 'n_estimators': 1650, 'max_depth': 36, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:28:47,085]\u001B[0m Trial 16 finished with value: 0.7993387686172073 and parameters: {'criterion': 'entropy', 'n_estimators': 1600, 'max_depth': 24, 'min_samples_leaf': 7}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:29:00,658]\u001B[0m Trial 17 finished with value: 0.811732813438098 and parameters: {'criterion': 'entropy', 'n_estimators': 1650, 'max_depth': 35, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:29:10,869]\u001B[0m Trial 18 finished with value: 0.8127140054536153 and parameters: {'criterion': 'log_loss', 'n_estimators': 1250, 'max_depth': 26, 'min_samples_leaf': 3}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n",
      "\u001B[32m[I 2022-08-20 00:29:19,669]\u001B[0m Trial 19 finished with value: 0.798449005629861 and parameters: {'criterion': 'log_loss', 'n_estimators': 1300, 'max_depth': 25, 'min_samples_leaf': 8}. Best is trial 14 with value: 0.8166661331917211.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наилучшие значения гиперпараметров {'criterion': 'entropy', 'n_estimators': 1650, 'max_depth': 37, 'min_samples_leaf': 2}\n",
      "f1_score на обучающем наборе: 0.82\n"
     ]
    }
   ],
   "source": [
    "def optuna_fr(trial)->float:\n",
    "    \"\"\"\n",
    "    Задаёт пространство гиперпараметров и обучает модель.\n",
    "    Возвращает F-меру модели на обучающей выборке.\n",
    "    \"\"\"\n",
    "    criterion_opt = trial.suggest_categorical('criterion',\n",
    "                                              ['gini', 'entropy', 'log_loss'])\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 2000, step=50)\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 40)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 15)\n",
    "    model_opt = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                       criterion=criterion_opt,\n",
    "                                       max_depth=max_depth,\n",
    "                                       min_samples_leaf=min_samples_leaf,\n",
    "                                       n_jobs=-1,\n",
    "                                       random_state=42)\n",
    "    model_opt.fit(X_train, y_train)\n",
    "    score = cross_val_score(model_opt, X_train, y_train, scoring=\"f1\",\n",
    "                            n_jobs=-1).mean()\n",
    "    return score\n",
    "\n",
    "\n",
    "# Ищем лучшую комбинацию гиперпараметров при максимизации метрики:\n",
    "study = optuna.create_study(study_name='Test run', direction=\"maximize\")\n",
    "study.optimize(optuna_fr, n_trials=20)\n",
    "print(\"Наилучшие значения гиперпараметров {}\".format(study.best_params))\n",
    "print(\"f1_score на обучающем наборе: {:.2f}\".format(study.best_value))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-мера на тестовой части: 0.839\n"
     ]
    }
   ],
   "source": [
    "# модель с полученными гиперпараметрами\n",
    "optuna_fr_best = RandomForestClassifier(**study.best_params, n_jobs=-1,\n",
    "                                    random_state=42)\n",
    "optuna_fr_best.fit(X_train, y_train)\n",
    "# метрика на тестовых данных\n",
    "f1_score_optuna_fr = on_da_screen(optuna_fr_best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Вывод\n",
    "Таким образом, можно сделать следующие заключение -  для решения задач необходимо использовать различные алгоритмы, так как изначально трудно \n",
    "предположить, что сработает лучше. В данном случае наилучший результат был достигнут с помощью ансамбля случайного леса при с оптимальными гиперпараметрами от Optuna. "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}